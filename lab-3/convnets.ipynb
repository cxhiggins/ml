{
 "cells": [
  {
   "source": [
    "# Lab 3\n",
    "## Convolutional Neural Networks (CNNs)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, initializers, optimizers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisations\n",
    "\n",
    "Let us write some helper functions to initialise weights and biases. We'll initialise weights as Gaussian random variables with mean 0 and variance 0.0025. For biases we'll initialise everything with a constant 0.1. This is because we're mainly going to be using ReLU non-linearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "source": [
    "We'll also initialize the MNIST data set for training and testing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):\n",
    "  return tf.one_hot(labels, len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = one_hot(train_labels)\n",
    "test_labels = one_hot(test_labels)"
   ]
  },
  {
   "source": [
    "### Visualization\n",
    "Let us visualise the first 16 data points from the MNIST training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.imshow(train_images[i].reshape(28, 28), cmap='Greys_r')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Let's define the model. The model is defined as follows:\n",
    "\n",
    "* An input that is 728 dimensional vector. \n",
    "* Reshape the input as 28x28x1 images (only 1 because they are grey scale) \n",
    "* A convolutional layer with 25 filters of shape 12x12x1 and a ReLU non-linearity (with stride (2, 2) and no padding)\n",
    "* A convolutional layer with 64 filters of shape 5x5x25 and a ReLU non-linearity (with stride (1, 2) and padding to maintain size)\n",
    "* A max_pooling layer of shape 2x2\n",
    "* A fully connected layer taking all the outputs of the max_pooling layer to 1024 units and ReLU nonlinearity\n",
    "* A fully connected layer taking 1024 units to 10 no activation function (the softmax non-linearity will be included in the loss function rather than in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(\n",
    "    filters=25,\n",
    "    kernel_size=(12, 12),\n",
    "    strides=(2,2),\n",
    "    activation='relu',\n",
    "    input_shape=(28, 28, 1),\n",
    "    padding='valid',  # uses no padding\n",
    "    kernel_initializer=initializers.RandomNormal(mean=0, stddev=0.05),\n",
    "    use_bias=True,\n",
    "    bias_initializer=initializers.Constant(0.1)\n",
    "))\n",
    "model.add(layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(5, 5),\n",
    "    strides=(1,1),\n",
    "    activation='relu',\n",
    "    padding='same'  # pads to maintain width and height\n",
    "))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(\n",
    "    units=1024,\n",
    "    activation='relu'\n",
    "))\n",
    "model.add(layers.Dropout(\n",
    "    rate=0.2\n",
    "))\n",
    "model.add(layers.Dense(\n",
    "    units=10,\n",
    "    activation='softmax'\n",
    "))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the flow of data through the model\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "source": [
    "Now that the model sequence has been defined, we can train our model on the provided training data. The model fitting has been specified so that it uses 5000 iterations of batches of size 50 â€” this is separated into 50 epochs, with 100 iterations each."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=False,\n",
    "        label_smoothing=0, \n",
    "        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n",
    "    ),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_images.reshape(60000, 28, 28, 1),\n",
    "    train_labels,\n",
    "    batch_size=50,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=50, \n",
    "    validation_data=(test_images.reshape(10000, 28, 28, 1), test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images.reshape(10000, 28, 28, 1),  test_labels, verbose=2)"
   ]
  },
  {
   "source": [
    "With minibatches of size 50 and 5000 iterations (50 epochs of 100 iterations each), the accuracy with this model is 0.9891999959945679!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(image_data):\n",
    "  assert(len(image_data) >= 16)\n",
    "  predictions = list(map(np.argmax, model.predict(image_data[:16].reshape(16, 28, 28, 1))))\n",
    "\n",
    "  fig = plt.figure()\n",
    "  for i in range(16):\n",
    "      ax = fig.add_subplot(4, 4, i + 1)\n",
    "      ax.set_xticks(())\n",
    "      ax.set_yticks(())\n",
    "      ax.imshow(image_data[i].reshape(28, 28), cmap='Greys_r')\n",
    "      ax.set_xlabel(predictions[i])\n",
    "\n",
    "  fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "visualize_prediction(test_images[16:32])"
   ]
  },
  {
   "source": [
    "### Unused Skeleton Code\n",
    "The code below was provided for TensorFlow users. I have not touched this code, since I found the Keras submodule of TensorFlow to be easier to use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "x_ = tf.reshape(x, [-1, 28, 28, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Define the first convolution layer here\n",
    "# TODO\n",
    "# W_conv1 =\n",
    "# b_conv1 = \n",
    "# h_conv1 = \n",
    "\n",
    "# Define the second convolution layer here\n",
    "# W_conv2 = \n",
    "# b_conv2 = \n",
    "# h_conv2 = \n",
    "\n",
    "# Define maxpooling\n",
    "# h_pool2 = \n",
    "\n",
    "# All subsequent layers will be fully connected ignoring geometry so we'll flatten the layer\n",
    "# Flatten the h_pool2_layer (as it has a multidimensiona shape) \n",
    "# h_pool2_flat = \n",
    "\n",
    "# Define the first fully connected layer here\n",
    "# W_fc1 = \n",
    "# b_fc1 = \n",
    "# h_fc1 = \n",
    "\n",
    "# Use dropout for this layer (should you wish)\n",
    "# keep_prob = tf.placeholder(tf.float32)\n",
    "# h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# The final fully connected layer\n",
    "# W_fc2 = \n",
    "# b_fc2 = \n",
    "# y_conv = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function, Accuracy and Training Algorithm\n",
    "\n",
    "* We'll use the cross entropy loss function. The loss function is called `tf.nn.cross_entropy_with_logits` in tensorflow.\n",
    "\n",
    "* Accuray is simply defined as the fraction of data correctly classified.\n",
    "\n",
    "* For training you should use the AdamOptimizer (read the documentation) and set the learning rate to be 1e-4. You are welcome, and in fact encouraged, to experiment with other optimisation procedures and learning rates. \n",
    "\n",
    "* (Optional): You may even want to use different filter sizes once you are finished with experimenting with what is asked in this practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the cross entropy loss function \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "\n",
    "# And classification accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# And the Adam optimiser\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us visualise the first 16 data points from the MNIST training data\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.imshow(mnist.train.images[i].reshape(28, 28), cmap='Greys_r')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a tf session and run the optimisation algorithm\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(3000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # TODO\n",
    "    # Write the optimisation code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy on the test set\n",
    "# print ('Test accuracy: %g' % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Filters\n",
    "\n",
    "We'll now visualise all the 32 filters in the first convolution layer. As they are each of shape 12x12x1, they may themselves be viewed as greyscale images. Visualising filters in further layers is more complicated and involves modifying the neural network. See the paper (https://arxiv.org/pdf/1311.2901.pdf) by Matt Zeiler and Rob Fergus if you are interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the filters in the first convolutional layer\n",
    "with sess.as_default():\n",
    "    W = W_conv1.eval()\n",
    "\n",
    "# Add code to visualise filters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying image patches that activate the filters\n",
    "\n",
    "For this part you'll find the 12 patches in the test-set that activate each of the first 5 filters that maximise the activation for that filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H =  sess.run(h_conv1, feed_dict={x: mnist.test.images})\n",
    "\n",
    "\n",
    "# Add code to visualise patches in the test set that find the most result in \n",
    "# the highest activations for filters 0, ... 4"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}